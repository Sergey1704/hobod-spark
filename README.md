# Домашнее задание по Spark

## Введение

Предлагается решить задачу 1 используя 2 различных Spark API:
1. RDD API,
2. DataFrame API.

а также задачу 2, используя любое из 2 API.

Цель задания: убедиться насколько DataFrame API быстрее и удобнее чем RDD.

## Задача 1

### Исходные данные

Данные лежат в HDFS.
* Полный датасет: `/data/twitter/twitter_sample.txt`
* Частичная выборка: `/data/twitter/twitter_sample_small.txt`

Формат данных: `user_id \t follower_id`

### Условие задачи

Дан ориентированный граф. Необходимо найти длину кратчайшего пути между  вершинами 12 и 34 графа, реализовав алгоритм "Поиск в ширину". Если кратчайших путей несколько, выведите первый.

Данную задачу нужно:
* решить двумя способами - с помощью RDD и с помощью DF API
* замерить CPU time (не wall time поскольку он измеряет время с учётом загруженности кластера).

Для сдачи задачи нужно не только пройти тесты, но и прислать 2 числа - замеры времени работы каждого способа.  
Обратите внимание на критерий остановки алгоритма. В рамках оптимизации вы можете остановить программу раньше, чем закончится поиск в ширину т.к. нам достаточно одно пути.

Выходной формат: последовательность вершин (учитывая начало и конец), разделенных запятой, без пробелов. Например, путь «12 -> 42 -> 34» должен быть напечатан как: `12,42,34`.

## Задача 2

### Исходные данные

Данные лежат в HDFS.
* Cтатьи Википедии: `/data/wiki/en_articles_part`.  
Формат данных: `article ID <tab> article text`
* Cписок стоп-слов: `/data/wiki/stop_words_en-xpo6.txt`.  
Формат данных: одно стоп-слово на строчку

Задача состоит в извлечении коллокаций. Это комбинации слов, которые часто встречаются вместе. Например, «High school» или «Roman Empire». Чтобы найти совпадения, нужно использовать метрику NPMI (нормализованная точечная взаимная информация).

PMI двух слов a и b определяется как:  
![equation](https://latex.codecogs.com/gif.latex?%5Csmall%20PMI%20%3D%20%5Cln%7B%28%5Cfrac%7BP%28ab%29%7D%7BP%28a%29%20%5Ccdot%20P%28b%29%7D%29%7D)  
, где P(ab) - вероятность двух слов, идущих подряд, а P(a) и P(b) - вероятности слов a и b соответственно.

NPMI вычисляется как:  
![equation](https://latex.codecogs.com/gif.latex?%5Csmall%20NPMI%20%3D%20-%5Cfrac%7BPMI%28ab%29%7D%7B%5Cln%7BP%28a%2Cb%29%7D%7D)  
Это нормализует величину в диапазон [-1; 1].

### Условие задачи

Найти самые популярные коллокации в Википедии. Обработка данных:
* При парсинге отбрасывайте все символы, которые не являются латинскими буквами: `text = re.sub("^\W+|\W+$", "", text)`
* приводим все слова к нижнему регистру;
* удаляем все стоп-слова (даже внутри биграммы т.к. “at evening” имеет ту же семантику что и “at the evening”);
* биграммы объединить символом нижнего подчеркивания "_";
* работаем только с теми биграммами, которые встретились не реже 500 раз (т.е. проводим все необходимые join'ы и считаем NPMI только для них).
* общее число слов и биграмм считать до фильтрации.

Для каждой биграммы посчитать NPMI и вывести на экран (в STDOUT) TOP-39 самых популярных коллокаций, отсортированных по убыванию значения NPMI. Само значение NPMI выводить не нужно.
